{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Code\n",
    "\n",
    "Let's break down the key steps needed to generate human faces using **StyleGAN2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Loading the Pre-trained Model**\n",
    "\n",
    "The pre-trained model is stored in a <mark>.pkl</mark> (pickle) file, which contains the generator and discriminator networks trained to produce high-quality human faces. You can load this model using the <mark>pretrained_networks</mark> module from the **StyleGAN2** repository.\n",
    "\n",
    "Here’s how you load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dnnlib\n",
    "import torch\n",
    "import pretrained_networks\n",
    "\n",
    "# Load the pre-trained model\n",
    "def load_stylegan2_model(model_path):\n",
    "    # Load the network from the pickle file\n",
    "    network_pkl = model_path\n",
    "    _, _, G, _ = pretrained_networks.load_networks(network_pkl)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- The <mark>load_networks</mark> function loads the StyleGAN2 model. The variable <mark>G</mark> is the generator network, which will be used to create new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Generating a Latent Vector**\n",
    "\n",
    "The generator in StyleGAN2 takes a latent vector as input. This vector represents random noise that the generator will transform into an image. A **latent vector** is typically a high-dimensional vector (e.g., 512-dimensional) sampled from a normal distribution.\n",
    "\n",
    "Here’s how we generate a random latent vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vector(latent_dim=512):\n",
    "    return torch.randn([1, latent_dim]).cuda()  # Ensure this tensor is on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- <mark>torch.randn([1, latent_dim])</mark> generates a random vector of size <mark>latent_dim</mark> (512 in this case). The <mark>.cuda()</mark> method moves the tensor to the GPU for faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Generating the Image (Human Face)**\n",
    "\n",
    "Once the latent vector is generated, we can pass it through the generator to produce an image. The generator’s <mark>synthesis</mark> function creates a new image from the latent vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_face(G, latent_vector):\n",
    "    with torch.no_grad():  # Disable gradient tracking for inference\n",
    "        generated_image = G.synthesis(latent_vector)\n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "<mark>G.synthesis(latent_vector)</mark> generates an image from the latent vector. We use <mark>torch.no_grad()</mark> to tell PyTorch not to compute gradients, as we’re only running inference (not training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Post-processing the Image**\n",
    "\n",
    "The generator returns the image as a tensor, and we need to process it into a format that can be displayed. We convert the tensor to a PIL image and resize it to a manageable size for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def process_image(tensor):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),  # Convert tensor to a PIL image\n",
    "        transforms.Resize((256, 256)),  # Resize for display\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(tensor.cpu()).resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "<mark>ToPILImage()</mark> converts the tensor to a PIL image, which can be easily manipulated and displayed. We also resize it to 256x256 pixels to ensure it fits on the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Putting It All Together**\n",
    "\n",
    "Now, let’s put everything together in a script that will load the model, generate a latent vector, generate the face, and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dnnlib\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pretrained_networks\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the pre-trained StyleGAN2 model\n",
    "def load_stylegan2_model(model_path):\n",
    "    network_pkl = model_path\n",
    "    _, _, G, _ = pretrained_networks.load_networks(network_pkl)\n",
    "    return G\n",
    "\n",
    "# Generate a random latent vector\n",
    "def generate_latent_vector(latent_dim=512):\n",
    "    return torch.randn([1, latent_dim]).cuda()\n",
    "\n",
    "# Generate a human face from the latent vector\n",
    "def generate_face(G, latent_vector):\n",
    "    with torch.no_grad():\n",
    "        generated_image = G.synthesis(latent_vector)\n",
    "    return generated_image\n",
    "\n",
    "# Post-process the image for display\n",
    "def process_image(tensor):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(tensor.cpu()).resize((256, 256))\n",
    "\n",
    "def main():\n",
    "    model_path = \"stylegan2-ffhq-config-f.pkl\"  # Path to the pre-trained model\n",
    "    G = load_stylegan2_model(model_path)  # Load the generator network\n",
    "    G = G.cuda()  # Ensure it runs on GPU\n",
    "    \n",
    "    latent_vector = generate_latent_vector()  # Generate a random latent vector\n",
    "    face_tensor = generate_face(G, latent_vector)  # Generate a face\n",
    "    face_image = process_image(face_tensor)  # Process the image for display\n",
    "    \n",
    "    face_image.show()  # Display the generated face\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we’ve walked through:\n",
    "\n",
    "- The theory behind **GANs** and how they generate new data (in this case, faces).\n",
    "- The code needed to load a pre-trained model, generate random latent vectors, pass them through the generator to create faces, and process the output to display it.\n",
    "\n",
    "#### Key Takeaways:\n",
    "\n",
    "- **Pre-trained models** like StyleGAN2 are often used in practice because training a GAN from scratch is computationally expensive.\n",
    "- **GANs use an adversarial process** to train the generator and discriminator. The generator aims to create realistic data, while the discriminator tries to identify fake data.\n",
    "- The **latent vector** is crucial as it drives the generation process, and modifying it can change the output image.\n",
    "\n",
    "Now you can generate realistic human faces with a few lines of code using a pre-trained StyleGAN2 model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
